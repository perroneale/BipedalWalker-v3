{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow import keras\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('BipedalWalker-v3')\n",
    "# #reset inizializza environment\n",
    "# #observation contiene le informazioni relative allo stato\n",
    "# observation = env.reset()\n",
    "# print(\"Observation: \", observation)\n",
    "# #numero di azioni che è possibile eseguire\n",
    "# print(\"Action space: \",env.action_space)\n",
    "# #numero di osservazioni che otteniamo dallo stato,\n",
    "# #ad esempio nel primo stato otteniamo quelle stampate precedentemente\n",
    "# print(\"Observation: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volgio utilizzare una DQN per addestrare il mio agente in grado di imparare a camminare. Per questo metodo avrò bisogno:\n",
    "1. Epsilon greedy selection rule: per selezionare l'azione, con probabilità epsilon andrò a selezionare un'azione random, nel resto dei casi l'azione sarà calcolata da una Neural Network.\n",
    "2. Neural Network: la rete avrà un input layer con 160 nodi (il numero di observation fornite dalo state) e nell'output layer vi saranno 4 nodi, pari al numero di azioni da eseguire, per ogni nodo vi sarà un valore che rappresenta il Q-value, e noi saremo interessati al valore massimo.\n",
    "3. Nel caso della DQN, per eseguire l'update dei parametri della rete, (Weight of the connections), andremo ad utilizzare una experience replay.\n",
    "4. Experience Replay: non è altro che un batch in cui andramo ad inserire tutti gli stati visitati con le relative informazioni fornite da env.step(action)\n",
    "5. Per eseguire l'update andremo, dopo un certo numero di timestep ad eseguire l'update tramite il policy gradient method.\n",
    "6. Devo discretizzare l'uscita, ogni nodo corrisponde a 0.1 (-1, -0.9, .. 1 per la prima azione e così via)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                1250      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                4284      \n",
      "=================================================================\n",
      "Total params: 8,084\n",
      "Trainable params: 8,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\perro\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODE = 1000\n",
    "NUM_TIMESTEP = 400\n",
    "LEARNING_RATE = 0.001 \n",
    "DISCOUNT_FACTOR = 0.9\n",
    "BATCH_SIZE = 32 #from the implementation of DQN for Atari games\n",
    "\n",
    "environment = gym.make('BipedalWalker-v3')\n",
    "#retrieve possible action that I can apply\n",
    "INPUT = [environment.observation_space.shape[0]]\n",
    "n_outputs = [environment.action_space.shape[0]]\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_function = keras.losses.mean_squared_error\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(50,activation=\"relu\", input_shape=INPUT),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(21*n_outputs[0])\n",
    "])\n",
    "\n",
    "#model.compile(optimizer=optimizer, loss=loss_function)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -1.0, 1: -0.9, 2: -0.8, 3: -0.7, 4: -0.6, 5: -0.5, 6: -0.4, 7: -0.3, 8: -0.2, 9: -0.1, 10: 0.0, 11: 0.1, 12: 0.2, 13: 0.3, 14: 0.4, 15: 0.5, 16: 0.6, 17: 0.7, 18: 0.8, 19: 0.9, 20: 1.0, 21: -1, 22: -0.9, 23: -0.8, 24: -0.7, 25: -0.6, 26: -0.5, 27: -0.4, 28: -0.3, 29: -0.2, 30: -0.1, 31: 0.0, 32: 0.1, 33: 0.2, 34: 0.3, 35: 0.4, 36: 0.5, 37: 0.6, 38: 0.7, 39: 0.8, 40: 0.9, 41: 1.0, 42: -1, 43: -0.9, 44: -0.8, 45: -0.7, 46: -0.6, 47: -0.5, 48: -0.4, 49: -0.3, 50: -0.2, 51: -0.1, 52: 0.0, 53: 0.1, 54: 0.2, 55: 0.3, 56: 0.4, 57: 0.5, 58: 0.6, 59: 0.7, 60: 0.8, 61: 0.9, 62: 1.0, 63: -1, 64: -0.9, 65: -0.8, 66: -0.7, 67: -0.6, 68: -0.5, 69: -0.4, 70: -0.3, 71: -0.2, 72: -0.1, 73: 0.0, 74: 0.1, 75: 0.2, 76: 0.3, 77: 0.4, 78: 0.5, 79: 0.6, 80: 0.7, 81: 0.8, 82: 0.9, 83: 1.0}\n",
      "[-1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "dictionarie =  {}\n",
    "start =  - 1.0\n",
    "for i in range(84):\n",
    "    dictionarie[i] = start\n",
    "    if start == 1:\n",
    "        start = -1\n",
    "    else:\n",
    "        start = round(start + 0.1,1)\n",
    "print(dictionarie)\n",
    "\n",
    "list_value = [-1.0, -0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "print(list_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_Q_values_each_action(Q_values):\n",
    "    best_index_action = []\n",
    "    for i in range(4):\n",
    "        #print(\"Range: from {} to {} \".format(21*i, 21*(i+1)) )\n",
    "        best_index_action.append(np.argmax(Q_values[0][21*i:21*(i+1)]) + 21*i)\n",
    "        #print(best_index_action)\n",
    "    return np.array(best_index_action, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I use DQN so I need Q-learning and Neural Network\n",
    "#For Q-learning I use epsilon greedy seletion rule, I need a function to do that\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    action = [0,0,0,0]\n",
    "    index  = [0,0,0,0]\n",
    "    if np.random.rand() < epsilon:\n",
    "        #I pick a random action\n",
    "        for i in range(4):\n",
    "            action[i] = random.choice(list_value)\n",
    "            #print(\"Action = \",action)\n",
    "            for k,v in dictionarie.items():\n",
    "                if v == action[i]:\n",
    "                    index[i] = k + 21*i\n",
    "                    #print(\"Index \", index)\n",
    "                    break\n",
    "        return action, index\n",
    "    else:\n",
    "        #Starting from the state and using the network I generate the qvalues\n",
    "        #associated to the action and pick the highest\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        #print(Q_values)\n",
    "        index_max_qvalues = get_max_Q_values_each_action(Q_values)\n",
    "        #varie alternative\n",
    "        for i in range(4):\n",
    "            action[i] = dictionarie[index_max_qvalues[i]]\n",
    "        #print(\"Action \", action)\n",
    "        return action, index_max_qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(observation, epsilon, environment):\n",
    "    #1. seleziono azione con epsilon greedy policy\n",
    "    action, index = epsilon_greedy(observation, epsilon)\n",
    "    state, reward, done, info = environment.step(action)\n",
    "    #2. aggiungo nel replpay buffer\n",
    "    experience_replay.append([observation, reward, done, index, state])\n",
    "    return state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_replay = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience():\n",
    "    random_index = np.random.randint(low=0, high=len(experience_replay), size=BATCH_SIZE)\n",
    "    #recupero dall'experience replay i vettori corrispondenti agli indici random\n",
    "    batch= [experience_replay[index] for index in random_index]\n",
    "    batch_state, batch_reward, batch_done, batch_action_index, batch_next_state = [np.array([batch_element[field] for batch_element in batch]) for field in range(5)]\n",
    "    return batch_state, batch_reward, batch_done, batch_action_index, batch_next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_max_Q_values_each_action(Q_values):\n",
    "    max1, max2, max3,max4 = [],[],[],[]\n",
    "    for q in Q_values:\n",
    "        max1.append(np.max(q[0:22]))\n",
    "        max2.append(np.max(q[21:42]))\n",
    "        max3.append(np.max(q[41:63]))\n",
    "        max4.append(np.max(q[62:84]))\n",
    "    \n",
    "    max1 = np.array(max1)\n",
    "    max2 = np.array(max2)\n",
    "    max3 = np.array(max3)\n",
    "    max4 = np.array(max4)\n",
    "    Q_next = np.array([max1, max2,max3,max4])\n",
    "    return Q_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_index_single_action(batch, action):\n",
    "    index_first_action = np.empty((32,1), dtype=np.int16)\n",
    "    for i in range(32):\n",
    "        index_first_action[i] = batch[i][action]\n",
    "    return index_first_action.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network():\n",
    "    exp = get_experience()\n",
    "    batch_state, batch_reward, batch_done, batch_action_index, batch_next_state = exp\n",
    "    #Q LEARNING FORMULA\n",
    "    #Q_t = Q_t + alpha(Rt+1 + gamma(maxQt+1) - Q_t)\n",
    "    #I need Q_t+1\n",
    "    Q_next_state = model.predict(batch_next_state)\n",
    "    Q_next_state_per_action = get_batch_max_Q_values_each_action(Q_next_state) #4 vettori da 32 elementi\n",
    "    delta = []\n",
    "    loss = []\n",
    "    for i in range(4):\n",
    "        delta.append(batch_reward + (1 - batch_done)*DISCOUNT_FACTOR*(Q_next_state_per_action[i]))\n",
    "    \n",
    "    delta = np.array(delta)\n",
    "    #print(\"shape\", delta.shape) #4,32\n",
    "    index_first_action = batch_index_single_action(batch_action_index, 0)\n",
    "    index_second_action = batch_index_single_action(batch_action_index,1)\n",
    "    index_third_action = batch_index_single_action(batch_action_index,2)\n",
    "    index_fourth_action = batch_index_single_action(batch_action_index,3)\n",
    "    tot_index =  np.array([index_first_action, index_second_action, index_third_action, index_fourth_action])\n",
    "    #print(tot_index)\n",
    "    #print(\"shape tot_index \", tot_index.shape)\n",
    "    tot_index = tot_index.reshape(4,32)\n",
    "    #print(tot_index)\n",
    "    #print(\"shape tot_index \", tot_index.shape)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        all_Q_values = model(batch_state)\n",
    "        for i in range(4):\n",
    "            mask = tf.one_hot(tot_index[0][i], 21*n_outputs[0])\n",
    "            #print(\"Mask: \",mask)\n",
    "            Q_values = tf.reduce_sum(all_Q_values*mask, axis=1, keepdims=True)\n",
    "            loss.append(tf.reduce_mean(loss_function(delta[0][i],Q_values)))\n",
    "    \n",
    "    #print(\"Loss\",loss)\n",
    "    for i in range(4):\n",
    "        grads = tape.gradient(loss[i], model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode:  0\n",
      "Cumulative Reward episode 0 = -18.954857562740383 \n",
      "Episode 0 completed.\n",
      "Current episode:  1\n",
      "Cumulative Reward episode 1 = -16.888183726662252 \n",
      "Episode 1 completed.\n",
      "Current episode:  2\n",
      "Cumulative Reward episode 2 = -103.18986475681129 \n",
      "Episode 2 completed.\n",
      "Current episode:  3\n",
      "Cumulative Reward episode 3 = -109.99622193980242 \n",
      "Episode 3 completed.\n",
      "Current episode:  4\n",
      "Cumulative Reward episode 4 = -101.6607135113541 \n",
      "Episode 4 completed.\n",
      "Current episode:  5\n",
      "Cumulative Reward episode 5 = -24.133149330383542 \n",
      "Episode 5 completed.\n",
      "Current episode:  6\n",
      "Cumulative Reward episode 6 = -99.9153119497329 \n",
      "Episode 6 completed.\n",
      "Current episode:  7\n",
      "Cumulative Reward episode 7 = -22.81874897988725 \n",
      "Episode 7 completed.\n",
      "Current episode:  8\n",
      "Cumulative Reward episode 8 = -112.2652514123431 \n",
      "Episode 8 completed.\n",
      "Current episode:  9\n",
      "Cumulative Reward episode 9 = -109.51657390672912 \n",
      "Episode 9 completed.\n",
      "Current episode:  10\n",
      "Cumulative Reward episode 10 = -99.64583477690121 \n",
      "Episode 10 completed.\n",
      "Current episode:  11\n",
      "Cumulative Reward episode 11 = -102.0623253110343 \n",
      "Episode 11 completed.\n",
      "Current episode:  12\n",
      "Cumulative Reward episode 12 = -23.206302230981724 \n",
      "Episode 12 completed.\n",
      "Current episode:  13\n",
      "Cumulative Reward episode 13 = -12.251935918270663 \n",
      "Episode 13 completed.\n",
      "Current episode:  14\n",
      "Cumulative Reward episode 14 = -104.2806515501257 \n",
      "Episode 14 completed.\n",
      "Current episode:  15\n",
      "Cumulative Reward episode 15 = -21.074217192385813 \n",
      "Episode 15 completed.\n",
      "Current episode:  16\n",
      "Cumulative Reward episode 16 = -122.96572677715011 \n",
      "Episode 16 completed.\n",
      "Current episode:  17\n",
      "Cumulative Reward episode 17 = -114.89503880792819 \n",
      "Episode 17 completed.\n",
      "Current episode:  18\n",
      "Cumulative Reward episode 18 = -26.524940476704394 \n",
      "Episode 18 completed.\n",
      "Current episode:  19\n",
      "Cumulative Reward episode 19 = -19.526464088892702 \n",
      "Episode 19 completed.\n",
      "Current episode:  20\n",
      "Cumulative Reward episode 20 = -97.47490934177799 \n",
      "Episode 20 completed.\n",
      "Current episode:  21\n",
      "Cumulative Reward episode 21 = -116.32631328143204 \n",
      "Episode 21 completed.\n",
      "Current episode:  22\n",
      "Cumulative Reward episode 22 = -31.450652700825767 \n",
      "Episode 22 completed.\n",
      "Current episode:  23\n",
      "Cumulative Reward episode 23 = -101.28925778165919 \n",
      "Episode 23 completed.\n",
      "Current episode:  24\n",
      "Cumulative Reward episode 24 = -118.53287709846262 \n",
      "Episode 24 completed.\n",
      "Current episode:  25\n",
      "Cumulative Reward episode 25 = -128.27971383022617 \n",
      "Episode 25 completed.\n",
      "Current episode:  26\n",
      "Cumulative Reward episode 26 = -101.35131213257436 \n",
      "Episode 26 completed.\n",
      "Current episode:  27\n",
      "Cumulative Reward episode 27 = -116.35261309856338 \n",
      "Episode 27 completed.\n",
      "Current episode:  28\n",
      "Cumulative Reward episode 28 = -16.255421955546353 \n",
      "Episode 28 completed.\n",
      "Current episode:  29\n",
      "Cumulative Reward episode 29 = -26.838623920644693 \n",
      "Episode 29 completed.\n",
      "Current episode:  30\n",
      "Cumulative Reward episode 30 = -103.88662988679421 \n",
      "Episode 30 completed.\n",
      "Current episode:  31\n",
      "Cumulative Reward episode 31 = -114.41518821845887 \n",
      "Episode 31 completed.\n",
      "Current episode:  32\n",
      "Cumulative Reward episode 32 = -102.5025853302228 \n",
      "Episode 32 completed.\n",
      "Current episode:  33\n",
      "Cumulative Reward episode 33 = -26.09677611563988 \n",
      "Episode 33 completed.\n",
      "Current episode:  34\n",
      "Cumulative Reward episode 34 = -17.840449615187307 \n",
      "Episode 34 completed.\n",
      "Current episode:  35\n",
      "Cumulative Reward episode 35 = -115.64942573714455 \n",
      "Episode 35 completed.\n",
      "Current episode:  36\n",
      "Cumulative Reward episode 36 = -114.62804187123825 \n",
      "Episode 36 completed.\n",
      "Current episode:  37\n",
      "Cumulative Reward episode 37 = -111.91240997730878 \n",
      "Episode 37 completed.\n",
      "Current episode:  38\n",
      "Cumulative Reward episode 38 = -100.92453712680775 \n",
      "Episode 38 completed.\n",
      "Current episode:  39\n",
      "Cumulative Reward episode 39 = -17.611948969679172 \n",
      "Episode 39 completed.\n",
      "Current episode:  40\n",
      "Cumulative Reward episode 40 = -103.93703169755526 \n",
      "Episode 40 completed.\n",
      "Current episode:  41\n",
      "Cumulative Reward episode 41 = -105.27846451712139 \n",
      "Episode 41 completed.\n",
      "Current episode:  42\n",
      "Cumulative Reward episode 42 = -120.84834954840665 \n",
      "Episode 42 completed.\n",
      "Current episode:  43\n",
      "Cumulative Reward episode 43 = -115.6997306233288 \n",
      "Episode 43 completed.\n",
      "Current episode:  44\n",
      "Cumulative Reward episode 44 = -114.91591977178318 \n",
      "Episode 44 completed.\n",
      "Current episode:  45\n",
      "Cumulative Reward episode 45 = -105.43800647455467 \n",
      "Episode 45 completed.\n",
      "Current episode:  46\n",
      "Cumulative Reward episode 46 = -11.579208160740125 \n",
      "Episode 46 completed.\n",
      "Current episode:  47\n",
      "Cumulative Reward episode 47 = -24.103821215349946 \n",
      "Episode 47 completed.\n",
      "Current episode:  48\n",
      "Cumulative Reward episode 48 = -112.12320760550897 \n",
      "Episode 48 completed.\n",
      "Current episode:  49\n",
      "Cumulative Reward episode 49 = -29.343355893485775 \n",
      "Episode 49 completed.\n",
      "Current episode:  50\n",
      "Cumulative Reward episode 50 = -101.98470452278343 \n",
      "Episode 50 completed.\n",
      "Current episode:  51\n",
      "Cumulative Reward episode 51 = -109.37269770199322 \n",
      "Episode 51 completed.\n",
      "Current episode:  52\n",
      "Cumulative Reward episode 52 = -31.723822708871257 \n",
      "Episode 52 completed.\n",
      "Current episode:  53\n",
      "Cumulative Reward episode 53 = -101.16722806684996 \n",
      "Episode 53 completed.\n",
      "Current episode:  54\n",
      "Cumulative Reward episode 54 = -121.02886701710199 \n",
      "Episode 54 completed.\n",
      "Current episode:  55\n",
      "Cumulative Reward episode 55 = -96.6794717684875 \n",
      "Episode 55 completed.\n",
      "Current episode:  56\n",
      "Cumulative Reward episode 56 = -99.05301347922223 \n",
      "Episode 56 completed.\n",
      "Current episode:  57\n",
      "Cumulative Reward episode 57 = -23.925506544442424 \n",
      "Episode 57 completed.\n",
      "Current episode:  58\n",
      "Cumulative Reward episode 58 = -19.0349476670384 \n",
      "Episode 58 completed.\n",
      "Current episode:  59\n",
      "Cumulative Reward episode 59 = -99.54547784012767 \n",
      "Episode 59 completed.\n",
      "Current episode:  60\n",
      "Cumulative Reward episode 60 = -100.96581103434289 \n",
      "Episode 60 completed.\n",
      "Current episode:  61\n",
      "Cumulative Reward episode 61 = -17.799692066299283 \n",
      "Episode 61 completed.\n",
      "Current episode:  62\n",
      "Cumulative Reward episode 62 = -120.4174359437986 \n",
      "Episode 62 completed.\n",
      "Current episode:  63\n",
      "Cumulative Reward episode 63 = -121.17632210312846 \n",
      "Episode 63 completed.\n",
      "Current episode:  64\n",
      "Cumulative Reward episode 64 = -96.09706549692837 \n",
      "Episode 64 completed.\n",
      "Current episode:  65\n",
      "Cumulative Reward episode 65 = -116.27870419907757 \n",
      "Episode 65 completed.\n",
      "Current episode:  66\n",
      "Cumulative Reward episode 66 = -26.474519047524282 \n",
      "Episode 66 completed.\n",
      "Current episode:  67\n",
      "Cumulative Reward episode 67 = -11.370541759820146 \n",
      "Episode 67 completed.\n",
      "Current episode:  68\n",
      "Cumulative Reward episode 68 = -22.092676939921944 \n",
      "Episode 68 completed.\n",
      "Current episode:  69\n",
      "Cumulative Reward episode 69 = -11.50037729876591 \n",
      "Episode 69 completed.\n",
      "Current episode:  70\n",
      "Cumulative Reward episode 70 = -104.4902665617713 \n",
      "Episode 70 completed.\n",
      "Current episode:  71\n",
      "Cumulative Reward episode 71 = -119.60873763528468 \n",
      "Episode 71 completed.\n",
      "Current episode:  72\n",
      "Cumulative Reward episode 72 = -132.3514082819962 \n",
      "Episode 72 completed.\n",
      "Current episode:  73\n",
      "Cumulative Reward episode 73 = -110.60090334129495 \n",
      "Episode 73 completed.\n",
      "Current episode:  74\n",
      "Cumulative Reward episode 74 = -112.96811686034202 \n",
      "Episode 74 completed.\n",
      "Current episode:  75\n",
      "Cumulative Reward episode 75 = -20.22132452571341 \n",
      "Episode 75 completed.\n",
      "Current episode:  76\n",
      "Cumulative Reward episode 76 = -103.47774708747168 \n",
      "Episode 76 completed.\n",
      "Current episode:  77\n",
      "Cumulative Reward episode 77 = -16.21141687603465 \n",
      "Episode 77 completed.\n",
      "Current episode:  78\n",
      "Cumulative Reward episode 78 = -102.54062275377872 \n",
      "Episode 78 completed.\n",
      "Current episode:  79\n"
     ]
    }
   ],
   "source": [
    "done_external = False\n",
    "\n",
    "for episode in range(NUM_EPISODE):\n",
    "    cumulative_reward = 0\n",
    "    print(\"Current episode: \",episode)\n",
    "    state = environment.reset()\n",
    "    for timestep in range(NUM_TIMESTEP):\n",
    "        epsilon = max(1 - episode/200, 0.1)\n",
    "        state, reward, done, info = perform_action(state, epsilon, environment)\n",
    "        cumulative_reward += reward\n",
    "        if reward == -100: #il robot è caduto\n",
    "            break;\n",
    "        if done:\n",
    "            print(\"Game solved!!\")\n",
    "            done_external = done\n",
    "            break\n",
    "        if episode > 3: #per avere un numero di elementi maggiore della dimensione del batchsize\n",
    "            update_network()\n",
    "        environment.render()\n",
    "        \n",
    "    if done_external:\n",
    "        break\n",
    "    print(\"Cumulative Reward episode {} = {} \".format(episode, cumulative_reward))\n",
    "    print(\"Episode {} completed.\".format(episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
